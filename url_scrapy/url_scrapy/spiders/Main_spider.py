import scrapy,re
import urlparse
from bs4 import BeautifulSoup
i=0
class GithubSP(scrapy.Spider):
    global commit
    name = "All_commit_url"
    allowed_domains = ["github.com"]
    git_new_urls=[]
    f = open("All_Url.txt", "r")
    for line in f:
        if re.search(r"http[^\"]*", line):
            if re.search(r"github", line):
                if re.search(r"commit", line):
                    url = re.findall(r"http[^\"]*", line)[0]
                    git_new_urls.append(str(url))
    start_urls=git_new_urls

    def parse(self,response):
        html = response.body
        page_url = "https://github.com"
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.find('strong', itemprop="name").find("a")
        new_next_full_url = urlparse.urljoin(page_url, links['href'])
        yield scrapy.Request(url=new_next_full_url, callback=self.parse_commithead)

    def parse_commithead(self,response):
        html=response.body
        page_url = "https://github.com"
        soup = BeautifulSoup(html, 'html.parser')
        if soup.find('li', class_="commits").find("a"):
            commithead_urls = soup.find('li', class_="commits").find("a")
            new_next_full_url = urlparse.urljoin(page_url, commithead_urls['href'])
            yield scrapy.Request(url=new_next_full_url, callback=self.parse_page)

    def parse_page(self,response):
        html = response.body
        page_url = "https://github.com"
        soup = BeautifulSoup(html, 'html.parser')
        #if soup.find('div', class_="pagination").find("a", rel="nofollow", href=re.compile(r"(.*)after(.*)")):
        compage = soup.find('div', class_="pagination").find("a", rel="nofollow", href=re.compile(r"(.*)after(.*)"))
        new_next_full_url = urlparse.urljoin(page_url, compage['href'])
        yield scrapy.Request(url=new_next_full_url, callback=self.parse_commit)



    def parse_commit(self,response):
        global commit
        html = response.body
        fout = open("Output_url.txt", "a")
        soup = BeautifulSoup(html, 'html.parser')
        new_commit_urls = []
        page_url = "https://github.com"
        a = soup.find_all('a', class_="sha btn btn-outline BtnGroup-item")
        if len(a) != 0:
            for i in range(len(a)):
                new_commit_urls.append(urlparse.urljoin(page_url, a[i]["href"]))
                fout.write(new_commit_urls[-1]+"\n")
        if soup.find('div', class_="pagination"):
            compage = soup.find('div', class_="pagination").find("a", rel="nofollow", href=re.compile(r"(.*)after(.*)"))
            new_next_full_url = urlparse.urljoin(page_url, compage['href'])
            yield scrapy.Request(url=new_next_full_url, callback=self.parse_page)
